{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GFwqquUc-dMC",
        "P3mcVMor4-4i",
        "cX0a5jzy8Xe3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP0gtU8N4LAM",
        "outputId": "fe65c126-4bcb-4738-a6f0-0ecbe58ecdb9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUllfvxa4Seh"
      },
      "source": [
        "!pip install scikit-learn==0.24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGT4pPF44cXk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# regression model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# metrics packages\n",
        "from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_squared_error\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFwqquUc-dMC"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8NME3PT-fI9"
      },
      "source": [
        "def hyper_parameter_search(data, X, baseline_start, baseline_end, regressor):\n",
        "  # get date range for baseline and select data\n",
        "  time_base = pd.date_range(start=baseline_start, end=baseline_end, freq='2MS')\n",
        "  data_base = data[time_base]\n",
        "\n",
        "  # get climate data for baseline and stressor\n",
        "  X_base = X.loc[time_base]\n",
        "\n",
        "  # initialize best params matrix\n",
        "  best_params = pd.DataFrame(index=data_base.index, columns=['params','best_score', 'mape', 'rmse'])\n",
        "\n",
        "  # normalize data\n",
        "  X_base_scaled = scaler.fit_transform(X_base)\n",
        "\n",
        "  # predict consumption during the stressor for each account by Random Forrest Regressor\n",
        "  for i, row in data_base.iterrows():\n",
        "    y = row.array\n",
        "    model = regressor.fit(X_base_scaled, y)\n",
        "    y_pred = model.predict(X_base_scaled)\n",
        "    mape = mean_absolute_percentage_error(y_true=y, y_pred=y_pred)\n",
        "    rmse = mean_squared_error(y_true=y, y_pred=y_pred, squared=False)\n",
        "\n",
        "    best_params.loc[i] = (model.best_params_, model.best_score_,mape, rmse)\n",
        "\n",
        "  return best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO8ccpp-tfFe"
      },
      "source": [
        "def run_model(data, climate_data, baseline_start, baseline_end,  best_params):\n",
        "  # get date range for baseline and select data\n",
        "  time_base = pd.date_range(start=baseline_start, end=baseline_end, freq='2MS')\n",
        "  data_base = data[time_base]\n",
        "\n",
        "  # get climate data for baseline and stressor\n",
        "  X_base = climate_data.loc[time_base]\n",
        "  X_base_scaled = scaler.fit_transform(X_base)\n",
        " \n",
        "\n",
        "  # initialize quantile predictions\n",
        "  quantiles = [0.1, 0.2, 0.5, 0.8, 0.9]\n",
        "  prediction_rf = pd.DataFrame(index=data_base.index, columns=['params', 'q0.1','q0.2', 'q0.5', 'q0.8', 'q0.9', 'y_true', 'date'])\n",
        "\n",
        "  # predict consumption during the stressor for each account by Random Forrest Regressor\n",
        "  for i, row in data_base.iterrows():\n",
        "    print(i)\n",
        "    y = row.array\n",
        "    params = best_params['params'].loc[i]\n",
        "    regressor = RandomForestRegressor(**params)\n",
        "    regressor.fit(X_base_scaled, y)\n",
        "    preds = [rf_quantile(regressor, X_base_scaled, q) for q in quantiles]\n",
        "    prediction_rf.loc[i] = (params, preds[0], preds[1], preds[2], preds[3], preds[4], y.to_numpy(), time_base)\n",
        "\n",
        "\n",
        "  return prediction_rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxlNF7-XuaTx"
      },
      "source": [
        "def rf_quantile(m, X, q):\n",
        "    rf_preds = []\n",
        "    for estimator in m.estimators_:\n",
        "        rf_preds.append(estimator.predict(X))\n",
        "    rf_preds = np.array(rf_preds).transpose()  # One row per record.\n",
        "    return np.percentile(rf_preds, q * 100, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3mcVMor4-4i"
      },
      "source": [
        "# data loading and preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ppkqxx_7vNA"
      },
      "source": [
        "# define customer group\n",
        "customers = 'MD'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o16OGFP48EG"
      },
      "source": [
        "# load consumption data, climatic and employment features\n",
        "target_data = pd.read_pickle('/content/drive/MyDrive/Stanford-TUBerlin/CodePaper/Consumption_Matrices/{}_accounts_actual_usage'.format(customers))\n",
        "features = pd.read_csv('/content/drive/MyDrive/Stanford-TUBerlin/CodePaper/climate_economic_features' , index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmgNfc6c5OEK"
      },
      "source": [
        "# process features\n",
        "features.index= pd.to_datetime(features.index)\n",
        "features['month'] = features.index.month_name()\n",
        "# create binary variable for month\n",
        "X = pd.get_dummies(features, columns = ['month'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_data"
      ],
      "metadata": {
        "id": "PWeM5_XF8mei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX0a5jzy8Xe3"
      },
      "source": [
        "# Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0eZqb6x8aZo",
        "outputId": "3ff7bec3-588d-44ca-ca08-be0d5f09ab0f"
      },
      "source": [
        "#### random grid ####\n",
        "# Number of trees in random forest\n",
        "n_estimators = [50, 100, 150, 250, 500]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth =[]# [int(x) for x in np.linspace(10, 100, num = 8)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf}\n",
        "\n",
        "print(random_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': [50, 100, 150, 250, 500], 'max_features': ['auto', 'sqrt'], 'max_depth': [None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78pWywHB9ef6"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# custom scoring\n",
        "scoring = {'MAPE': make_scorer(mean_absolute_percentage_error)}#, 'MSE': make_scorer(mean_squared_error)}\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3,verbose=2,  random_state=142, n_jobs = -1)#, scoring=make_scorer(mean_absolute_percentage_error))\n",
        "\n",
        "# initialize scaler to normalize features\n",
        "scaler = MinMaxScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-0WcN0c_Tkg"
      },
      "source": [
        "# start cross validation\n",
        "\n",
        "# shuffel customers and select 100 \n",
        "#data_100 = target_data.sample(n=10, random_state=256354)\n",
        "\n",
        "#best_params_100 = hyper_parameter_search(data_100, X, '1/1/2002', '11/1/2007', rf_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kh8h1hGAUFV"
      },
      "source": [
        "#best_params_100.mape.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start hyperparameter search with cross validation\n",
        "# divide data into sets of 500 customers\n",
        "\n",
        "best_params_MD = hyper_parameter_search(target_data, X, '1/1/2002', '11/1/2007', rf_random)\n",
        "best_params_MD.to_pickle('/content/drive/MyDrive/Stanford-TUBerlin/CodePaper/Hyperparams_MD/best_params_MD')"
      ],
      "metadata": {
        "id": "Kbf1o4ljBUV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}